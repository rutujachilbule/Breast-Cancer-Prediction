# -*- coding: utf-8 -*-
"""Breast Cancer Prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9m7MAwDUFLMpJRWBiy02VNVwldRTs21

# **Build a logistic regression model and predict if a given specimen is benign or malignant, using the Breast Cancer dataset using python**

## 1.**Data**
"""

# import dependencies
# data cleaning and manipulation 
import pandas as pd
import numpy as np

#data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
from sklearn.preprocessing import StandardScaler

import sklearn.linear_model as skl_lm 
from sklearn import preprocessing

# initialize some package settings
sns.set(style="whitegrid", color_codes=True, font_scale=1.3)

# %matplotlib inline

# read in the data and check the first 5 rows
df = pd.read_csv('/content/data.csv')
df.head()

df.shape #checking dimension of data

df = df.drop('id', axis = 1)

# general summary of the dataframe
df.info()

# remove the 'Unnamed: 32' column
df = df.drop('Unnamed: 32', axis=1)

# visualize distribution of classes 
plt.figure(figsize=(8, 4))
sns.countplot(df['diagnosis'], palette='RdBu')

# count number of observations in each class
benign, malignant = df['diagnosis'].value_counts()
print('Number of cells labeled Benign: ', benign)
print('Number of cells labeled Malignant : ', malignant)
print('')
print('% of cells labeled Benign', round(benign / len(df) * 100, 2), '%')
print('% of cells labeled Malignant', round(malignant / len(df) * 100, 2), '%')

"""# **2. The Variables**"""

# first, drop all "worst" columns
cols = ['radius_worst', 
        'texture_worst', 
        'perimeter_worst', 
        'area_worst', 
        'smoothness_worst', 
        'compactness_worst', 
        'concavity_worst',
        'concave points_worst', 
        'symmetry_worst', 
        'fractal_dimension_worst']
df = df.drop(cols, axis=1)

# then, drop all columns related to the "perimeter" and "area" attributes
cols = ['perimeter_mean',
        'perimeter_se', 
        'area_mean', 
        'area_se']
df = df.drop(cols, axis=1)

# lastly, drop all columns related to the "concavity" and "concave points" attributes
cols = ['concavity_mean',
        'concavity_se', 
        'concave points_mean', 
        'concave points_se']
df = df.drop(cols, axis=1)

# verify remaining columns
df.columns

"""# **3. The Model**"""

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X = df
y = df['diagnosis']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)

# Create a string for the formula
cols = df.columns.drop('diagnosis')
formula = 'diagnosis ~ ' + ' + '.join(cols)
print(formula, '\n')

# Run the model and report the results
import statsmodels.api as sm
import statsmodels.formula.api as smf 
model = smf.glm(formula=formula, data=X_train, family=sm.families.Binomial())
logistic_fit = model.fit()

print(logistic_fit.summary())

"""# **4. The Prediction**"""

# predict the test data and show the first 5 predictions
predictions = logistic_fit.predict(X_test)
predictions[1:6]

# Convert these probabilities into nominal values and check the first 5 predictions again.
predictions_nominal = [ "M" if x < 0.5 else "B" for x in predictions]
predictions_nominal[1:6]

"""#**5. Model Evaluation**"""

from sklearn.metrics import confusion_matrix, classification_report, precision_score

print(classification_report(y_test, predictions_nominal, digits=3))
cfm = confusion_matrix(y_test, predictions_nominal)

true_negative = cfm[0][0]
false_positive = cfm[0][1]
false_negative = cfm[1][0]
true_positive = cfm[1][1]

print('Confusion Matrix: \n', cfm, '\n')

print('True Negative:', true_negative)
print('False Positive:', false_positive)
print('False Negative:', false_negative)
print('True Positive:', true_positive)
print('Correct Predictions', 
      round((true_negative + true_positive) / len(predictions_nominal) * 100, 1), '%')

from sklearn.metrics import accuracy_score, f1_score
print("Accuracy :", accuracy_score(y_test, predictions_nominal))